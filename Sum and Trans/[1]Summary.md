<br> 

---

# ğŸ“ƒ ëª©ì°¨
- [ğŸ“š ìš”ì•½](#ğŸ“š-ìš”ì•½)
    - [ğŸ“Œ ìš”ì•½ ëª¨ë¸ì˜ í•„ìš”ì„±](#ğŸ“Œ-ìš”ì•½-ëª¨ë¸ì˜-í•„ìš”ì„±)
    - [ğŸ“Œ ìš”ì•½ ëª¨ë¸ ì„ ì • ë°°ê²½](#ğŸ“Œ-ìš”ì•½-ëª¨ë¸-ì„ ì •-ë°°ê²½)
    - [ğŸ“Œ ìš”ì•½ ëª¨ë¸ ì‚¬ìš© ì½”ë“œ](#ğŸ“Œ-ìš”ì•½-ëª¨ë¸-ì‚¬ìš©-ì½”ë“œ)
    
<br>

- [ğŸ“ƒ ì°¸ê³ ìë£Œ](#ğŸ“ƒ-ì°¸ê³ ìë£Œ)

<br>
<br>
<br>

---
---
# ğŸ“š ìš”ì•½

## ğŸ“Œ ìš”ì•½ ëª¨ë¸ì˜ í•„ìš”ì„± 
1. Stable Diffusionì˜ ì…ë ¥ í”„ë¡¬í”„íŠ¸ì— í…ìŠ¤íŠ¸ ì „ë‹¬ ì‹œ ê¸¸ì´ ì œí•œ

    : í…ìŠ¤íŠ¸ ê¸°ë°˜ ì´ë¯¸ì§€ ìƒì„±(txt2img) ëª¨ë¸ Stable Diffusion ì˜ Promptì— ê°„ê²°í•œ ë‚´ìš©ì„ ì „ë‹¬í•˜ê¸° ìœ„í•¨

2. ë²ˆì—­ ëª¨ë¸ ì‚¬ìš© ì‹œ, ê¸€ì ìˆ˜ ì œí•œ 

    : Stable Diffusionì˜ PromptëŠ” ì˜ì–´ ì…ë ¥ë§Œ ì§€ì›í•¨

    **=>** ë²ˆì—­ ëª¨ë¸ì˜ input ê°’ ê¸¸ì´ ì œí•œìœ¼ë¡œ ì¸í•¨

<br>
<br>


## ğŸ“Œ ìš”ì•½ ëª¨ë¸ ì„ ì • ë°°ê²½
1. ìš”ì•½ ë°©ì‹ì€ í¬ê²Œ ì¶”ì¶œ ìš”ì•½, ìƒì„± ìš”ì•½ ë‘ ê°€ì§€ë¡œ ë‚˜ë‰¨


    - ì¶”ì¶œ ìš”ì•½ê³¼ ìƒì„± ìš”ì•½ì˜ íŠ¹ì§• ë¹„êµ<sup>[[1]](#footnote_1)</sup>

|   |**ì¶”ì¶œ ìš”ì•½**|**ìƒì„± ìš”ì•½**|
|:---:|:---:|:---:|
|ì…ë ¥ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ëŠ”ê°€?|O|X|
|ì •ë³´ ëˆ„ë½ ì •ë„|ë§ìŒ|ì ìŒ|
|ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ í•„ìš”ëŸ‰|ì ìŒ|ë§ìŒ|
|í•™ìŠµ ì‹œê°„ í•„ìš”ëŸ‰|ì ìŒ|ë§ìŒ|


&emsp;&emsp;âœ… ì •ë³´ì˜ ëˆ„ë½ì„ ì¤„ì´ê¸° ìœ„í•´ **ìƒì„± ìš”ì•½ ëª¨ë¸ ì±„íƒ**

<br>
<br>

2. SKTBrain ì–¸ì–´ ëª¨ë¸ ë¹„êµ<sup>[[2-1]](#footnote_2-1)</sup>
    
    -  KoGPT2
    
        : ë””ì½”ë”ë¡œë§Œ êµ¬ì„±ë˜ì–´ ìì—°ì–´ ìƒì„±ì—ë§Œ ê°•ì ì´ ìˆìŒ

    -  KoBART
    
        : ì¸ì½”ë”-ë””ì½”ë”ë¡œ êµ¬ì„±ë˜ì–´ ìì—°ì–´ì˜ ì´í•´ì™€ ìƒì„± ëª¨ë‘ì— ê°•ì ì´ ìˆê³ , ì¼ë°˜ì ìœ¼ë¡œ ìš”ì•½ì—ì„œ ê°€ì¥ ê°•ë ¥í•œ ëª¨ë¸ë¡œ ì•Œë ¤ì ¸ ìˆìŒ<sup>[[2-2]](#footnote_2-2)</sup>
    
&emsp;&emsp;âœ… **KoBART ì±„íƒ**

<br>
<br>

3. pre-trainedê°€ ì¶”ê°€ì ìœ¼ë¡œ ë” ëœ KoBART ëª¨ë¸ì„ ìš”ì•½ì— ì‚¬ìš©
- Huggingfaceì˜ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•˜ì—¬ ì•„ë˜ 3ê°€ì§€ ëª¨ë¸ ì‚¬ìš©
    1. `gogamza/kobart-summarization`<sup>[[3]](#footnote_3)</sup>

    2. `gogamza/kobart-base-v1`<sup>[[4]](#footnote_4)</sup>

    3. `ainize/kobart-news`<sup>[[5]](#footnote_5)</sup> 

- ì„¸ ê°€ì§€ ëª¨ë¸ ì¤‘ ìš”ì•½ ê²°ê³¼ ê¸¸ì´ê°€ íŠ¹ì • ëª¨ë¸ì´ í‰ê· ì ìœ¼ë¡œ ì§§ë‹¤ê¸° ë³´ë‹¤ ê° ë„ì„œ ë°ì´í„°ë§ˆë‹¤ ë‹¤ë¦„

&emsp;&emsp;âœ… **ê°ê°ì˜ ë„ì„œ ë°ì´í„°ì—ì„œ** ìš”ì•½ ì „ ë¬¸ì¥<sup>\**</sup>ê³¼ ì„¸ ê°€ì§€ ëª¨ë¸ ì‚¬ìš© ê²°ê³¼ ì¤‘ **ê°€ì¥ ì§§ì€ ìš”ì•½ ê²°ê³¼ë¥¼ ì¶”ì¶œí•˜ëŠ” ë°©ì‹ ì±„íƒ**

&emsp;&emsp;&emsp;<small>\** ê°„í˜¹ ìš”ì•½ ëª¨ë¸ì˜ ê²°ê³¼ë³´ë‹¤ ìš”ì•½ ì „ í…ìŠ¤íŠ¸ê°€ ë” ì§§ì€ ê²½ìš° ìˆìŒ</small>


<br>
<br>

## ğŸ“Œ ìš”ì•½ ëª¨ë¸ ì‚¬ìš© ì½”ë“œ
<br>

- **ì½”ë“œ ì‘ì„± í™˜ê²½: êµ¬ê¸€ colab CPU**

- êµ¬ê¸€ colabì˜ í•˜ë“œì›¨ì–´ ê°€ì†ê¸° GPU T4 ì‚¬ìš© ì‹œ ì†Œìš” ì‹œê°„ ëŒ€ëµ 20~30% ë‹¨ì¶•


<br>
<br>

### **í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°**
- ë„¤íŠ¸ì›Œí¬ ìƒíƒœì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ, ëŒ€ëµ 1ë¶„ 20ì´ˆê°„ ì‹¤í–‰
<small>

```python
# ìš”ì•½ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°

# ë¶ˆëŸ¬ì˜¤ëŠ” ì‹œê°„ ì¸¡ì •
%%time

!pip install transformers
!pip install transformers[sentencepiece]

from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration
import torch

import pandas as pd

# ìš”ì•½ ëª¨ë¸1 ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer1 = PreTrainedTokenizerFast.from_pretrained("gogamza/kobart-summarization")
model1 = BartForConditionalGeneration.from_pretrained("gogamza/kobart-summarization")

# ìš”ì•½ ëª¨ë¸2 ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer2 = PreTrainedTokenizerFast.from_pretrained('digit82/kobart-summarization')
model2 = BartForConditionalGeneration.from_pretrained('digit82/kobart-summarization')

# ìš”ì•½ ëª¨ë¸3 ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer3 = PreTrainedTokenizerFast.from_pretrained("ainize/kobart-news")
model3 = BartForConditionalGeneration.from_pretrained("ainize/kobart-news")
```
</small>

<br>

### **ë„ì„œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**
- `df` : ë¶ˆìš©ì–´ ì²˜ë¦¬ ë“±ì˜ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ê¹Œì§€ ë§ˆì¹œ ë„ì„œ ë°ì´í„°
<small>

```python
# ë„ì„œ ë°ì´í„°ê°€ csv íŒŒì¼ì¸ ê²½ìš°
df = pd.read_csv('íŒŒì¼ê²½ë¡œ ë° íŒŒì¼ëª…') 

# ë„ì„œ ë°ì´í„°ê°€ pickle íŒŒì¼ì¸ ê²½ìš°
df = pd.read_pickle('íŒŒì¼ê²½ë¡œ ë° íŒŒì¼ëª…')

# ë¶ˆëŸ¬ì˜¨ ë°ì´í„°í”„ë ˆì„ í™•ì¸
df
```
</small>
<br>

- ë°ì´í„° ë ˆì½”ë“œì™€ ì¹¼ëŸ¼ ê°œìˆ˜ í™•ì¸
<small>

```python
df.shape
```
</small>
<br>

- ìš”ì•½ì— ì‚¬ìš©í•  ì¹¼ëŸ¼ í™•ì¸
<small>

```python
list(df)
```
</small>
<br>

- ìš”ì•½ì— ì‚¬ìš©í•  ì¹¼ëŸ¼ì˜ ë°ì´í„° í™•ì¸
<small>

```python
df[['description']] # 'description' ì¹¼ëŸ¼ì„ ì‚¬ìš©í•  ê²½ìš°ì˜ ì½”ë“œ
```
</small>

<br>

### **ìš”ì•½ ì „ df**
- ê°„í˜¹ ìš”ì•½ ëª¨ë¸ì˜ ê²°ê³¼ë³´ë‹¤ ìš”ì•½ ì „ í…ìŠ¤íŠ¸ê°€ ë” ì§§ì€ ê²½ìš° ìˆì–´ ë¹„êµìš© df ìƒì„±
<small>

```python
temp = pd.DataFrame(columns=['text','sum_txt','len'])
temp['text'] = df['description']
temp['sum_txt'] = df['description']
temp['len'] = temp['text'].apply(lambda text: len(text))

temp[['sum_txt','len']]
```
</small>
<br>

### **ìš”ì•½**
### **ëª¨ë¸1ë¡œ ìš”ì•½**
- ë„¤íŠ¸ì›Œí¬ ìƒíƒœì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ ëŒ€ëµ 200ì ì´ë‚´ í…ìŠ¤íŠ¸ ë°ì´í„° 30ê°œ ì²˜ë¦¬ ì‹œ ì•½ 3ë¶„ ì´ìƒ ì†Œìš”
<small>

```python
# ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
%%time

temp1 = pd.DataFrame(columns=['text','sum_txt1','len1'])
temp1['text'] = df['description']

for i, text in enumerate(temp1['text']):

    input_ids = tokenizer1.encode(text)

    input_ids = [tokenizer1.bos_token_id] + input_ids + [tokenizer1.eos_token_id]
    input_ids = torch.tensor([input_ids])


    summary_text_ids = model1.generate(
        input_ids=input_ids,
        bos_token_id=model1.config.bos_token_id,
        eos_token_id=model1.config.eos_token_id,
        length_penalty=1.0, # ê¸¸ì´ì— ëŒ€í•œ penaltyê°’. 1ë³´ë‹¤ ì‘ì€ ê²½ìš° ë” ì§§ì€ ë¬¸ì¥ì„ ìƒì„±í•˜ë„ë¡ ìœ ë„í•˜ë©°, 1ë³´ë‹¤ í´ ê²½ìš° ê¸¸ì´ê°€ ë” ê¸´ ë¬¸ì¥ì„ ìœ ë„
        max_length=128,     # ìš”ì•½ë¬¸ì˜ ìµœëŒ€ ê¸¸ì´ ì„¤ì •
        min_length=32,      # ìš”ì•½ë¬¸ì˜ ìµœì†Œ ê¸¸ì´ ì„¤ì •
        num_beams=4,        # ë¬¸ì¥ ìƒì„±ì‹œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ íƒìƒ‰í•˜ëŠ” ì˜ì—­ì˜ ê°œìˆ˜
    )


    sum_txt = tokenizer1.decode(summary_text_ids[0], skip_special_tokens=True)

    temp1['sum_txt1'][i] = sum_txt
    temp1['len1'][i] = len(sum_txt)

# ìš”ì•½ ê²°ê³¼ í™•ì¸
temp1[['sum_txt1','len1']]
```
</small>
<br>

### **ëª¨ë¸2ë¡œ ìš”ì•½**
- ë„¤íŠ¸ì›Œí¬ ìƒíƒœì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ ëŒ€ëµ 200ì ì´ë‚´ í…ìŠ¤íŠ¸ ë°ì´í„° 30ê°œ ì²˜ë¦¬ ì‹œ ì•½ 3ë¶„ ì´ìƒ ì†Œìš”
<small>

```python
# ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
%%time

temp2 = pd.DataFrame(columns=['text','sum_txt2','len2'])
temp2['text'] = df['description']

for i, text in enumerate(temp2['text']):

    raw_input_ids = tokenizer2.encode(text)
    input_ids = [tokenizer2.bos_token_id] + raw_input_ids + [tokenizer2.eos_token_id]

    summary_ids = model2.generate(torch.tensor([input_ids]),
                                 num_beams=4,
                                 max_length=128,
                                 eos_token_id=1)
    sum_txt = tokenizer2.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)

    temp2['sum_txt2'][i] = sum_txt
    temp2['len2'][i] = len(sum_txt)

# ìš”ì•½ ê²°ê³¼ í™•ì¸
temp2[['sum_txt2','len2']]
```
</small>
<br>

### **ëª¨ë¸3ìœ¼ë¡œ ìš”ì•½**
- ë„¤íŠ¸ì›Œí¬ ìƒíƒœì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ ëŒ€ëµ 200ì ì´ë‚´ í…ìŠ¤íŠ¸ ë°ì´í„° 30ê°œ ì²˜ë¦¬ ì‹œ ì•½ 4ë¶„ ì´ë‚´ ì†Œìš”
<small>

```python
# ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
%%time

temp3 = pd.DataFrame(columns=['text','sum_txt3','len3'])
temp3['text'] = df['description']

for i, text in enumerate(temp3['text']):
    input_ids = tokenizer3.encode(text, return_tensors="pt")

    summary_text_ids = model3.generate(
        input_ids=input_ids,
        bos_token_id=model3.config.bos_token_id,
        eos_token_id=model3.config.eos_token_id,
        length_penalty=1.0, 
        max_length=128,
        min_length=32,
        num_beams=4,
    )

    sum_txt = tokenizer3.decode(summary_text_ids[0], skip_special_tokens=True)

    temp3['sum_txt3'][i] = sum_txt
    temp3['len3'][i] = len(sum_txt)

# ìš”ì•½ ê²°ê³¼ í™•ì¸
temp3[['sum_txt3','len3']]
```
</small>
<br>

### **ê°€ì¥ ì§§ì€ ìš”ì•½ë¬¸ìœ¼ë¡œ df ìƒì„±**
-  ëª¨ë¸ 1, 2, 3ì˜ ì‹¤í–‰ ê²°ê³¼ì™€ ìš”ì•½ ì „ ë¬¸ì¥ ê¸¸ì´ ë„¤ ê°€ì§€ ì¤‘ ê°€ì¥ ì§§ì€ ìš”ì•½ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ df ìƒì„±
<small>

```python
# ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
%%time

comp = temp.copy(deep=True) # warning ëœ¨ì§€ë§Œ ì´ìƒ ì—†ìŒ

# ê°€ì¥ ì§§ì€ ë°ì´í„° ë¹„êµ ì¶”ì¶œ
for i in range(len(comp)):
    comp['sum_txt'][i] = min(temp['text'][i], temp1['sum_txt1'][i], temp2['sum_txt2'][i], temp3['sum_txt3'][i], key=len)

# ì¶”ì¶œ ê²°ê³¼ í™•ì¸
comp['title'] = df['title']
comp['len'] = comp['sum_txt'].apply(len)
comp[['sum_txt','len']]
```
</small>
<br>

- ë²ˆì—­ì„ ê°™ì€ íŒŒì¼ì— ì´ì–´ì„œ ì§„í–‰í•œë‹¤ë©´ ì—¬ê¸°ì„œ ë¶€í„° ì‹¤í–‰ ìƒëµ
### **ê²°ê³¼ ë°ì´í„° ì €ì¥**
<small>

```python
# csv íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ê²½ìš°
comp = pd.read_csv('íŒŒì¼ê²½ë¡œ ë° íŒŒì¼ëª…') 
comp.to_csv('íŒŒì¼ê²½ë¡œ ë° íŒŒì¼ëª…',index=False)

# pickle íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ê²½ìš°
import pickle

comp.to_pickle('íŒŒì¼ê²½ë¡œ ë° íŒŒì¼ëª…')
```
</small>
<br>

- ì €ì¥ ê²°ê³¼ í™•ì¸
<small>

```python
# pickle íŒŒì¼ë¡œ ì €ì¥í•œ ê²½ìš°
df = pd.read_pickle('íŒŒì¼ê²½ë¡œ ë° íŒŒì¼ëª…')

# csv íŒŒì¼ë¡œ ì €ì¥í•œ ê²½ìš°
df = pd.read_csv('íŒŒì¼ê²½ë¡œ ë° íŒŒì¼ëª…') 

# ì €ì¥ ê²°ê³¼ ì¶œë ¥
df
```
</small>

<br>
<br>
<br> 

---
# ğŸ“ƒ ì°¸ê³ ìë£Œ
<b id="footnote_1">[[1]](#ğŸ“Œ-ìš”ì•½-ëª¨ë¸-ì„ ì •-ë°°ê²½)</b> <https://cocosy.tistory.com/78>

<b id="footnote_2-1">[[2-1]](#ğŸ“Œ-ìš”ì•½-ëª¨ë¸-ì„ ì •-ë°°ê²½)</b> <https://dspace.ewha.ac.kr/handle/2015.oak/262132>

<b id="footnote_2-2">[[2-2]](#ğŸ“Œ-ìš”ì•½-ëª¨ë¸-ì„ ì •-ë°°ê²½)</b> <https://chloelab.tistory.com/34>

<b id="footnote_3">[[3]](#ğŸ“Œ-ìš”ì•½-ëª¨ë¸-ì„ ì •-ë°°ê²½)</b> <https://huggingface.co/gogamza/kobart-summarization>

<b id="footnote_4">[[4]](#ğŸ“Œ-ìš”ì•½-ëª¨ë¸-ì„ ì •-ë°°ê²½)</b> <https://huggingface.co/gogamza/kobart-base-v1>

<b id="footnote_5">[[5]](#ğŸ“Œ-ìš”ì•½-ëª¨ë¸-ì„ ì •-ë°°ê²½)</b> <https://huggingface.co/ainize/kobart-news>

<br>
<br>

&emsp;â« [ëª©ì°¨ë¡œ ì˜¬ë¼ê°€ê¸°](#ğŸ“ƒ-ëª©ì°¨)
